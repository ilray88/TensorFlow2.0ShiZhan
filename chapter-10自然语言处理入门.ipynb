{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第 10 章 自然语言处理入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##10.1 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1 英文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '5', 'biggest', 'countries', 'by', 'population', 'in', '2017', 'are', 'china', 'india', 'united', 'states', 'indonesia', 'and', 'brazil']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.preprocessing.text as kp_text\n",
    "\n",
    "paragraph = \"The 5 biggest countries by population in 2017 are China, \" \\\n",
    "            \"India, United States, Indonesia, and Brazil.\"\n",
    "processed_text = kp_text.text_to_word_sequence(paragraph)\n",
    "print(processed_text)\n",
    "\n",
    "# 输出如下：\n",
    "# ['the', '5', 'biggest', 'countries', 'by', 'population', 'in', '2017',\n",
    "#  'are', 'china', 'india', 'united', 'states', 'indonesia', 'and', 'brazil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 10.1.2 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 10.2 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 10.2.1 独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "token2idx = {'人工智能': 0, '的': 1, '研究': 2, '可以': 3, '分为': 4, '几个': 5,\n",
    "             '技术': 6, '问题': 7, '是': 8, '一门': 9, '新': 10, '科学': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'人工智能': 1, '的': 2, '技术': 3, '研究': 4, '可以': 5, '分为': 6, '几个': 7, '问题': 8, '是': 9, '一门': 10, '新': 11, '科学': 12}\n",
      "[[1, 2, 4, 5, 6, 7, 3, 8], [1, 9, 10, 11, 2, 3, 12]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "samples = [\"人工智能 的 研究 可以 分为 几个 技术 问题\", \"人工智能 是 一门 新 的 技术 科学\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences(samples)\n",
    "print(sequence)\n",
    "\n",
    "print(to_categorical(sequence[0],\n",
    "                     num_classes=len(tokenizer.word_index)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 10.2.2 词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data/word2vec/sgns.weibo.bigram-char'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_10300/3635989190.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmodel_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'data/word2vec/sgns.weibo.bigram-char'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mw2v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgensim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mKeyedVectors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_word2vec_format\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mvector\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mw2v\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'猫咪'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvector\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\0ilraypan\\python_work\\env\\tensorflow2.0\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001B[0m in \u001B[0;36mload_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[0;32m   1629\u001B[0m         return _load_word2vec_format(\n\u001B[0;32m   1630\u001B[0m             \u001B[0mcls\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfvocab\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfvocab\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbinary\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbinary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0municode_errors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0municode_errors\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1631\u001B[1;33m             \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdatatype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdatatype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mno_header\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mno_header\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1632\u001B[0m         )\n\u001B[0;32m   1633\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\0ilraypan\\python_work\\env\\tensorflow2.0\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001B[0m in \u001B[0;36m_load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[0;32m   1953\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1954\u001B[0m     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"loading projection weights from %s\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1955\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'rb'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfin\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1956\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mno_header\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1957\u001B[0m             \u001B[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\0ilraypan\\python_work\\env\\tensorflow2.0\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001B[0m in \u001B[0;36mopen\u001B[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001B[0m\n\u001B[0;32m    193\u001B[0m         \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    194\u001B[0m         \u001B[0merrors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 195\u001B[1;33m         \u001B[0mnewline\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnewline\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    196\u001B[0m     )\n\u001B[0;32m    197\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfobj\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\0ilraypan\\python_work\\env\\tensorflow2.0\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001B[0m in \u001B[0;36m_shortcut_open\u001B[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001B[0m\n\u001B[0;32m    359\u001B[0m         \u001B[0mopen_kwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'errors'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 361\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_builtin_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlocal_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbuffering\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbuffering\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mopen_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    362\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    363\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'data/word2vec/sgns.weibo.bigram-char'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model_path = 'data/word2vec/sgns.weibo.bigram-char'\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(model_path)\n",
    "vector = w2v['猫咪']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 输出词向量词表前 20 个词语\n",
    "print(f\"word list: {w2v.index2word[:20]}\")\n",
    "\n",
    "# 输出词向量前 20 个词的向量，形状为 (20, 300)\n",
    "print(f\"word vectors: {w2v.vectors[:20]}\")\n",
    "\n",
    "vector = w2v['猫咪']\n",
    "print(f\"most similiar to 猫咪: \\n {w2v.similar_by_vector(vector)}\")\n",
    "\n",
    "print(f\"most similiar to 明星: \\n {w2v.most_similar('明星')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=1000,  # 标记个数，这个嵌入层总共能嵌入 999 个标记\n",
    "                            output_dim=128)  # 嵌入维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 10.2.3 从文本到词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tensorflow import keras\n",
    "\n",
    "model_path = 'data/word2vec/sgns.weibo.bigram-char'\n",
    "# 通常预训练词嵌入会比较大，加载很耗时耗内存资源，当内存资源有限或者需要快速实验时\n",
    "# 可以通过增加一个 limit 参数，只读取特定数量词向量来节省时间和资源\n",
    "# 下面代码只会加载最高频的 5000 个词的向量\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(model_path, limit=5000)\n",
    "\n",
    "token2index = {\n",
    "    '<PAD>': 0, # 由于我们用 0 补全序列，所以补全标记的索引必须为 0\n",
    "    '<UNK>': 1  # 新词标记的索引可以使任何一个，设置为 1 只是为了方便\n",
    "}\n",
    "\n",
    "# 我们遍历预训练词嵌入的词表，加入到我们的标记索引词典\n",
    "for token in w2v.index2word:\n",
    "    token2index[token] = len(token2index)\n",
    "\n",
    "# 初始化一个形状为 [标记总数，预训练向量维度] 的全 0 张量\n",
    "token_vector = np.zeros((len(token2index), w2v.vector_size))\n",
    "# 随机初始化 <UNK> 标记的张量\n",
    "token_vector[1] = np.random.rand(300)\n",
    "# 从索引 2 开始使用预训练的向量\n",
    "token_vector[2:] = w2v.vectors\n",
    "\n",
    "# 通过测试可以确定新构建的标记索引和标记向量映射关系没问题\n",
    "print(token_vector[token2index['成长']] == w2v['成长'])\n",
    "print(token_vector[token2index['市场']] == w2v['市场'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 使用处理过的预训练向量来初始化嵌入层\n",
    "L = keras.layers\n",
    "embedding_layer = L.Embedding(input_dim=len(token2index),  # 标记数量等于词表标记数量\n",
    "                              output_dim=w2v.vector_size,  # 嵌入维度等于预训练向量维度\n",
    "                              weights=[token_vector],        # 使用我们构建的权重张量\n",
    "                              trainable=False)             # 不可训练\n",
    "# 构建一个提取序列向量的模型\n",
    "model = keras.Sequential([\n",
    "    embedding_layer\n",
    "])\n",
    "# 我们不需要训练这个模型，所以这里的损失函数和优化器可以随意设定\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_token_2_idx(tokenized_sentence: List[str]) -> List[int]:\n",
    "    \"\"\"转换分词后的标记序列为标记索引序列\n",
    "\n",
    "    如果该标记在词表出现过使用其索引，如果词表不存在，则使用新词标记的索引来替代\n",
    "    Args:\n",
    "        tokenized_sentence: 分词后的序列\n",
    "    Returns:\n",
    "        标记索引序列\n",
    "    \"\"\"\n",
    "    token_ids = []\n",
    "    for token in tokenized_sentence:\n",
    "        token_ids.append(token2index.get(token, token2index['<UNK>']))\n",
    "    return token_ids\n",
    "\n",
    "tokenized_sentence = \"今天 天气 真 不错 ha\".split(' ')\n",
    "print(convert_token_2_idx(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence_index = convert_token_2_idx(tokenized_sentence)\n",
    "# 将序列索引包含一个样本的批量\n",
    "input_x = np.array([sentence_index])\n",
    "# 使用模型预测\n",
    "sentence_vector = model.predict(input_x)\n",
    "print(sentence_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-10237b69",
   "language": "python",
   "display_name": "PyCharm (git_jia)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}